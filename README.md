# Self-Prompt-Engineering Test Framework

A Semantic Kernel project demonstrating how automated testing enables AI coding assistants (like GitHub Copilot, Cursor, etc.) to **prompt-engineer themselves** by generating plugins, testing them, and self-correcting based on test failures.

## ğŸ¯ Core Concept

This project showcases a **self-correcting development workflow** where:

1. **AI tools generate plugins** based on natural language prompts
2. **Automated tests validate** that plugins work correctly and route properly
3. **Test failures guide corrections** - the AI can iteratively fix:
   - Function parameters and types
   - Function descriptions (`[Description]` attributes)
   - Function structure and naming
   - Plugin registration
4. **Human oversight ensures quality** - developers review generated code and test assertions

## ğŸ”„ How It Works

### The Self-Prompt-Engineering Cycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Prompt AI: "Add a weather plugin"                     â”‚
â”‚    â†“                                                      â”‚
â”‚ 2. AI generates WeatherPlugin.cs                         â”‚
â”‚    â†“                                                      â”‚
â”‚ 3. Tests run automatically (via .cursor/rules)             â”‚
â”‚    â†“                                                      â”‚
â”‚ 4. Test fails: "Expected tool not called"                  â”‚
â”‚    â†“                                                      â”‚
â”‚ 5. AI analyzes failure, fixes Description/parameters      â”‚
â”‚    â†“                                                      â”‚
â”‚ 6. Tests pass â†’ Plugin works correctly                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example: Weather Plugin Development

**Initial Prompt:**
> "Add a weather plugin that gets weather for a location"

**AI generates:**
- `WeatherPlugin.cs` with `GetWeather(string location)`
- Registers plugin in `StandardKernel.cs`
- Adds test case expecting `WeatherPlugin.GetWeather`

**Test runs and fails:**
```
âœ— FAILED
  Expected tool 'WeatherPlugin.GetWeather' was not called
  Called: GeolocationPlugin.GetCoordinates, WeatherPlugin.GetWeatherByCoordinates
```

**AI self-corrects:**
- Realizes weather needs coordinates first
- Updates test to expect: `GeolocationPlugin.GetCoordinates` â†’ `WeatherPlugin.GetWeatherByCoordinates`
- Or adjusts function signature to match actual routing behavior

**Tests pass** â†’ Plugin correctly integrated!

## ğŸ›¡ï¸ Preventing Routing Regressions

The test framework ensures that when new plugins are added:

1. **Existing plugins still route correctly** - Tests verify expected tools are called
2. **Forbidden tools aren't accidentally called** - Tests verify unwanted tools aren't invoked
3. **Response quality is maintained** - Tests verify responses contain required keywords
4. **Function call chains work** - Tests verify multi-step workflows (e.g., geolocation â†’ weather)

### Test Structure

Each test case validates:
- âœ… **Expected tools are called** - Ensures correct routing
- âœ… **Forbidden tools are NOT called** - Prevents routing regressions
- âœ… **Response contains keywords** - Ensures quality output

```csharp
new()
{
    Name = "Weather Query",
    Prompt = "What's the weather like in Seattle?",
    ExpectedToolsToCall = { 
        "GeolocationPlugin.GetCoordinates", 
        "WeatherPlugin.GetWeatherByCoordinates" 
    },
    ExpectedToolsNotToCall = { 
        "MathPlugin.Add", 
        "MathPlugin.Subtract" 
    },
    ResponseMustContain = { "Seattle", "weather" }
}
```

## ğŸ“ Project Structure

```
SKRoutingStyles/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ StandardKernel/          # Main kernel implementation
â”‚   â”‚   â””â”€â”€ StandardKernel.cs   # Plugin registration & chat interface
â”‚   â”œâ”€â”€ Plugins/                 # Plugin implementations
â”‚   â”‚   â”œâ”€â”€ MathPlugin.cs
â”‚   â”‚   â”œâ”€â”€ GeolocationPlugin.cs
â”‚   â”‚   â””â”€â”€ WeatherPlugin.cs
â”‚   â”œâ”€â”€ IntegrationTesterApp/    # Test framework
â”‚   â”‚   â”œâ”€â”€ Program.cs           # Test cases
â”‚   â”‚   â”œâ”€â”€ TestRunner.cs        # Test execution
â”‚   â”‚   â””â”€â”€ TestLogger.cs        # Function call capture
â”‚   â””â”€â”€ Utilities/               # Shared utilities
â”œâ”€â”€ .cursor/
â”‚   â””â”€â”€ rules/
â”‚       â””â”€â”€ StandardKernelDevGuide.mdc  # Auto-test rule
â””â”€â”€ run-tests.ps1                # Test runner script
```

## ğŸš€ Usage

### Running Tests

```powershell
# Run all tests
.\run-tests.ps1

# Run with verbose output
.\run-tests.ps1 --verbose
```

### Adding a New Plugin (AI-Assisted Workflow)

1. **Prompt your AI assistant:**
   > "Add a [feature] plugin that [does something]"

2. **AI generates:**
   - Plugin class in `src/Plugins/`
   - Registration in `StandardKernel.cs`
   - Test case in `IntegrationTesterApp/Program.cs`

3. **Tests run automatically** (via `.cursor/rules/StandardKernelDevGuide.mdc`)

4. **If tests fail:**
   - AI analyzes failures
   - AI fixes function descriptions, parameters, or test expectations
   - Tests re-run automatically

5. **Human review:**
   - âœ… Verify test assertions are correct
   - âœ… Ensure code quality and safety
   - âœ… Check that routing logic makes sense

### Example Test Output

```
Test: Weather Query... 
ğŸ’¬ [USER QUERY] What's the weather like in Seattle?
ğŸ’¡ [RESPONSE]
ğŸ”§ [FUNCTION CALL] GeolocationPlugin.GetCoordinates(location=Seattle)
âœ… [FUNCTION RESULT] GeolocationPlugin.GetCoordinates = Latitude: 47.60621, Longitude: -122.33207
ğŸ”§ [FUNCTION CALL] WeatherPlugin.GetWeatherByCoordinates(latitude=47.60621, longitude=-122.33207)
âœ… [FUNCTION RESULT] WeatherPlugin.GetWeatherByCoordinates = Current weather: Clear sky, Temperature: 42.8Â°F...
âœ“ PASSED
  Called: GeolocationPlugin.GetCoordinates, WeatherPlugin.GetWeatherByCoordinates
```

## ğŸ§  The Human Role

While AI handles code generation and self-correction, **human developers provide critical oversight**:

### What Humans Review

1. **Test Assertions**
   - Are the expected tools correct?
   - Do forbidden tools make sense?
   - Are response keywords appropriate?

2. **Code Quality**
   - Is the generated code safe?
   - Are there security concerns?
   - Does it follow project patterns?

3. **Routing Logic**
   - Does the function chain make sense?
   - Are there edge cases not covered?
   - Is the plugin design appropriate?

## âš ï¸ Important Notes

### This is a Basic Example

- **Iterations may be needed** - Complex plugins might require multiple test/feedback cycles
- **Not all edge cases covered** - Tests focus on happy paths and basic routing
- **Human review is essential** - Don't blindly trust generated code or test assertions

### Limitations

- Tests validate routing but not all business logic
- Some routing issues may require domain knowledge
- Complex multi-step workflows may need manual test design

## ğŸ”§ Technical Details

### Test Framework Components

1. **TestRunner** - Executes tests against the kernel
2. **TestLogger** - Captures function calls via logging
3. **TestCase** - Defines expected behavior
4. **TestResult** - Validates actual vs expected

### Logging System

The framework uses structured logging with emojis for clarity:
- ğŸ’¬ `[USER QUERY]` - Original user prompt
- ğŸ’¡ `[RESPONSE]` - Response start marker
- ğŸ”§ `[FUNCTION CALL]` - Plugin function invoked
- âœ… `[FUNCTION RESULT]` - Function return value

### Auto-Testing Rule

The `.cursor/rules/StandardKernelDevGuide.mdc` file automatically:
- Triggers tests when `src/StandardKernel/` files change
- Requires tests to pass before considering work complete
- Provides feedback for iterative improvement

## ğŸ“š Example Plugins

### MathPlugin
Basic arithmetic operations (Add, Subtract)

### GeolocationPlugin
Uses OpenMeteo API to convert location names to coordinates

### WeatherPlugin
Uses OpenMeteo API to get weather data from coordinates

**Key Insight:** This framework demonstrates that with proper test infrastructure, AI coding assistants can engage in a form of "prompt engineering themselves" - iteratively improving their output based on test feedback, with human oversight ensuring quality and correctness.
